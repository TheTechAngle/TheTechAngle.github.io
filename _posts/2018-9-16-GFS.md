---
layout: post
title: The Google File System Design Condensed 
---

* A GFS cluster consists of a single master, multiple chunk servers and is accessed by multiple clients. 
* Files are divided into fixed-size chunks. 
* Each chunk is identified by an immutable and globally unique 64 bit chunk handle assigned by the master at the time of chunk creation.

### The Master Server
* The master maintains all file system metadata. This includes the mapping from files to chunks, and the current locations of chunks. 
* The master does not keep a persistent record of which chunkservers have a replica of a given chunk. It simply polls chunkservers for that information. 
* It also controls system-wide activities such as chunklease management, garbage collection of orphaned chunks, and chunk migration between chunkservers. 
* The master periodically communicates with each chunkserver in HeartBeat messages to give it instructions and collect its state.

#### Why is the master not a bottleneck though?
Clients never read and write file data through the master. Instead, a client asks the master which chunkservers it should contact. It can ask for the addresses for as many chunk servers as it wants. It caches this information for a limited time, and only contacts the master again when it needs a new file or when the cached data has expired. 

### Why is the client’s chunk size so large? (64MB)
* Lesser number of times asking the master (Wouldn’t work as well if it was random access of different files, but its mostly sequential read writes)
* Because of one, lesser network overhead with a a TCP connection to 1 server
* Lesser metadata stored on master
* Problems with such a large chunk space?
  * Space wasted - Lazy space allocation to prevent wasting space as much
  * Small files might have just the one chunk- have to have a higher replication factor so that one server doesn’t become a hotspot

### Metadata
* The operation log contains a historical record of critical metadata changes
* The master stores three major types of metadata: the file and chunk namespaces, the mapping from files to chunks, and the locations of each chunk’s replicas. All metadata is kept in the master’s memory. Less than 64 bytes of metadata for each 64 MB chunk. 
* Since the operation log is critical, we must store it reliably and not make changes visible to clients until metadata changes are made persistent


